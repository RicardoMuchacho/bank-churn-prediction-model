import pickle
import pandas as pd
import kagglehub
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler

#Download Dataset
path = kagglehub.dataset_download("radheshyamkollipara/bank-customer-churn")

# Loading Data
data = pd.read_csv(f"{path}/Customer-Churn-Records.csv")
data.info()

df = data.drop(columns=['RowNumber', 'CustomerId','Surname', 'Complain'])
df.head()

card_types = {'DIAMOND': 4, 'PLATINUM': 3, 'GOLD': 2, 'SILVER': 1}
gender_types = {'Female': 1, 'Male': 0}

df['Card Type'] = df['Card Type'].map(card_types)
df['Gender'] = df['Gender'].map(gender_types)

df.head()

#Get dummies encoding
dummies = pd.get_dummies(df['Geography'], prefix='Geography')

df = pd.concat([df, dummies], axis=1)
df = df.drop(columns=['Geography'])
df.head()

training_data = df.drop(columns=['Exited'])
testing_data = df['Exited']
X_train, X_test, Y_train, Y_test = train_test_split(training_data, testing_data, test_size=0.20, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

cv = KFold(n_splits=5, shuffle=True, random_state=42)

def crossValidate(model, param_grid):
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring='roc_auc')
    grid_search.fit(X_train, Y_train)
    
    return grid_search.best_score_, grid_search.best_params_

#hyperparameters
param_grid_lr = {
    'penalty': ['l1', 'l2'],
    'C': [0.1, 0.5, 1, 10, 20],
    'solver': ['liblinear']
}
param_grid_knn = {
    'n_neighbors': list(range(5, 105, 5)),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}
param_grid_dt = {
    'max_depth': [10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'criterion': ['gini', 'entropy']
}
param_grid_xgb = {
    'max_depth': [2, 4, 6, 8, 10],
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2, 0.3],
}
param_grid_rf = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [10, 20, 30, None],
}

lr_model = LogisticRegression(max_iter=10000)
auc_lr, best_params_lr = crossValidate(lr_model, param_grid_lr)

print(f"LogisticRegression AUC = {auc_lr}")
print(f"Best parameters found: {best_params_lr}")

knn_model = KNeighborsClassifier()
auc_knn, best_params_knn = crossValidate(knn_model, param_grid_knn)
  
print(f"KNeighbors AUC = {auc_knn}")
print(f"Best parameters found: {best_params_knn}")

dt_model = DecisionTreeClassifier(random_state=15)
auc_dt, best_params_dt = crossValidate(dt_model, param_grid_dt)
  
print(f"DecisionTree AUC = {auc_dt}")
print(f"Best parameters found: {best_params_dt}")

xgb_model = xgb.XGBClassifier()

with open("xgb_model", "wb") as f:
    pickle.dump(xgb_model, f)

auc_xgb, best_params_xgb = crossValidate(xgb_model, param_grid_xgb)
  
print(f"xgboost AUC = {auc_xgb}")
print(f"Best parameters found: {best_params_xgb}")

rf_model = RandomForestClassifier(random_state=42)
auc_rf, best_params_rf = crossValidate(rf_model, param_grid_rf)
  
print(f"RandomForest AUC = {auc_rf}")
print(f"Best parameters found: {best_params_rf}")

results = {
    "lreg": auc_lr,
    "xgb": auc_xgb,
    "knn": auc_knn,
    "rf": auc_rf,
    "tr": auc_dt
}

sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)

for i, (model, auc) in enumerate(sorted_results, 1):
    print(f"{i}. {model} = {auc}")

results_percentage = {model: score * 100 for model, score in sorted_results}

models = list(results_percentage.keys())
scores = list(results_percentage.values())

plt.figure(figsize=(8, 6))
plt.barh(models, scores, color='skyblue')
plt.xlabel('AUC Scores (%)')
plt.ylabel('Models')
plt.title('Comparison of Model AUC Scores')
plt.xlim(0, 100)
plt.gca().invert_yaxis()



